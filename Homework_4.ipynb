{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework 4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RfXFvkuWjqWE",
        "outputId": "8f6b6c6d-00f9-4d14-c918-f966a7090db7"
      },
      "source": [
        "!unzip \"/content/trainingandtestdata.zip\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/trainingandtestdata.zip\n",
            "replace testdata.manual.2009.06.14.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: testdata.manual.2009.06.14.csv  \n",
            "replace training.1600000.processed.noemoticon.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: training.1600000.processed.noemoticon.csv  y\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1ZRYkKCikQ5"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import os, random,io\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4QXvQLuj7Vf"
      },
      "source": [
        "name_col = [\"Polarity\",\"id\",\"date\",\"query\",\"user\",\"text\"]\n",
        "\n",
        "Complete_data = pd.read_csv(\"/content/training.1600000.processed.noemoticon.csv\",names=name_col , encoding='latin-1')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AtzWybtmPPv"
      },
      "source": [
        "positive_set = Complete_data[Complete_data[\"Polarity\"] == 0 ]\n",
        "neagtive_set = Complete_data[Complete_data[\"Polarity\"] == 4 ]\n",
        "\n",
        "PN_Combined = pd.concat([positive_set,neagtive_set])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5e9gR1csUA2"
      },
      "source": [
        "new_col = [\"Polarity\",\"text\"]\n",
        "PN_Clean = pd.DataFrame(data=PN_Combined[new_col])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNG2j_CpyUIC"
      },
      "source": [
        "PN_Clean[\"Polarity\"]=PN_Clean[\"Polarity\"].replace(4,1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN25IHW8o5p4"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzYithyzhnma"
      },
      "source": [
        "1. Take the positive and the negative tweets only. Use Sklearn to split the dataset in 80% training, 20% testing splits. Provide a nicely formatted summary of these splits, containing their size) (15 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZEqwZFNnII6"
      },
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(PN_Clean.text,PN_Clean.Polarity,test_size=0.2,random_state=2361)\n",
        "#Train_data,Testing_data = train_test_split(PN_Clean,test_size=0.20,random_state=2361)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VurAAeN9mBjX",
        "outputId": "1e62520d-bc57-4ee7-fcf3-927957b06a97"
      },
      "source": [
        "print(\"The size of training data set for tweet text is\",x_train.size)\n",
        "print(\"The size of testing data set for tweet text is\",x_test.size)\n",
        "\n",
        "print(\"The size of training data set for tweet Polarity is\",y_train.size)\n",
        "print(\"The size of testing data set for tweet Polarity is\",y_test.size)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of training data set for tweet text is 1280000\n",
            "The size of testing data set for tweet text is 320000\n",
            "The size of training data set for tweet Polarity is 1280000\n",
            "The size of testing data set for tweet Polarity is 320000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3XxWStPhsvP"
      },
      "source": [
        "2. Use the code from the previous classes to build the following models (15 points): \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j73Q-UzkpP2p"
      },
      "source": [
        "a) SVM using TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpuupIxwpWuY"
      },
      "source": [
        "SVM_model = make_pipeline(TfidfVectorizer(),LinearSVC()) \n",
        "SVM_model = RF_model.fit(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHAn8jeLpSsb"
      },
      "source": [
        "b) NaiveBayes using TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgzyn8PBpbRt"
      },
      "source": [
        "NV_model = make_pipeline(TfidfVectorizer(),MultinomialNB()) \n",
        "NV_model = RF_model.fit(x_train,y_train)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT5BYcaGpVGN"
      },
      "source": [
        "c) Random Forest using TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gNvwdwZ5B6v"
      },
      "source": [
        "RF_model = make_pipeline(TfidfVectorizer(),RandomForestClassifier(max_depth=20)) \n",
        "RF_model = RF_model.fit(x_train,y_train)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fG4sx0Qhy-_"
      },
      "source": [
        "3. Use the code from the LSTM class to build a classifier for negative and positive sentiment tweets. Train the model with the training data split. Once the model is built, test it with the testing data split. Display the classifier report for this evaluation. Answer the following question: What can you say about the performance of this model? (40 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTSTQhKMzmLh"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from numpy import array\n",
        "import logging"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q44-Zp1sPoaJ"
      },
      "source": [
        "def LSTM_Classify(PN_Clean,x_train,x_test,y_train,y_test):\n",
        "\n",
        "  print(\"Building a LSTM Classification Model\")\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(PN_Clean.text)\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  vocad_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "  result = [len(x) for x in PN_Clean.text]\n",
        "\n",
        "  review_length = len(max((PN_Clean.text), key=len))\n",
        "\n",
        "  x_train = pad_sequences(tokenizer.texts_to_sequences(x_train),maxlen = review_length)\n",
        "  x_test = pad_sequences(tokenizer.texts_to_sequences(x_test),maxlen = review_length)\n",
        "\n",
        "  print(\"Shape Training Review Data: \" + str(x_train.shape))\n",
        "  print(\"Shape Training Class Data: \" + str(x_test.shape))\n",
        "\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  model.add(\n",
        "      tf.keras.layers.Embedding(\n",
        "          input_dim = vocad_size, # The size of our vocabulary \n",
        "          output_dim = 32, # Dimensions to which each words shall be mapped\n",
        "          input_length = review_length # Length of input sequences\n",
        "      )\n",
        "  )\n",
        "\n",
        "  model.add(\n",
        "      tf.keras.layers.Dropout(\n",
        "          rate=0.25 # Randomly disable 25% of neurons\n",
        "      )\n",
        "  )\n",
        "\n",
        "  model.add(\n",
        "      tf.keras.layers.LSTM(\n",
        "          units=32 # 32 LSTM units in this layer\n",
        "      )\n",
        "  )\n",
        "\n",
        "  model.add(\n",
        "      tf.keras.layers.Dropout(\n",
        "          rate=0.25 # Randomly disable 25% of neurons\n",
        "      )\n",
        "  )\n",
        "\n",
        "\n",
        "  model.add(\n",
        "      tf.keras.layers.Dense(\n",
        "          units=1, # Single unit\n",
        "          activation='sigmoid' # Sigmoid activation function (output from 0 to 1)\n",
        "      )\n",
        "  )\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(\n",
        "      loss=tf.keras.losses.binary_crossentropy, # loss function\n",
        "      optimizer=tf.keras.optimizers.Adam(), # optimiser function\n",
        "      metrics=['accuracy']) # reporting metric\n",
        "\n",
        "  # Display a summary of the models structure\n",
        "  model.summary()\n",
        "\n",
        "  history = model.fit(\n",
        "    x_train, y_train,           \n",
        "    batch_size=256, \n",
        "    epochs=3, \n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        "  ) \n",
        "\n",
        "\n",
        "  return history\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3lwxqXaltfg"
      },
      "source": [
        "LSTM_Model_1 = LSTM_Classify(PN_Clean,x_train,x_test,y_train,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW2nM6jJLMKL"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "predicted_classes = model.predict_classes(x_test)\n",
        "print(classification_report(y_test, predicted_classes, target_names=class_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2vAxRI_h72z"
      },
      "source": [
        "4. Compare all models together in terms of Precision, Recall and F1 score. Put all of these numbers in a nicely formatted dataframe. Answer the following questions: Which model performs the best? Why do you think this is? What do you think you can do to improve performance? (30 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZeHyQvcS0xN"
      },
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(PN_Clean.text,PN_Clean.Polarity,test_size=0.2,random_state=2361)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty2JlHT4MM2I"
      },
      "source": [
        "label_1=NV_model.predict(x_test)\n",
        "label_2=RF_model.predict(x_test)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdEvV0KsMq0G"
      },
      "source": [
        "label_3=SVM_model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-aklolTNOOl"
      },
      "source": [
        "f1Score1 = []\n",
        "precision1 = []\n",
        "recall1 = []\n",
        "\n",
        "columns=['Model_Name','Vectorizer','Precision','Recall','Macro_F1-score']\n",
        "Modelnames = ['Naive_Bayes','RandomForest','Support_Vector_Machines']\n",
        "\n",
        "def scorefunction(label,testdata):\n",
        "  temp= sklearn.metrics.f1_score(label,y_test,average='macro')\n",
        "  temp3 = sklearn.metrics.precision_score(y_test,label)\n",
        "  temp4 = sklearn.metrics.recall_score(y_test,label)\n",
        "\n",
        "  f1Score1.append(temp)\n",
        "  precision1.append(temp3)\n",
        "  recall1.append(temp4)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gasjSqNYNjDh"
      },
      "source": [
        "import sklearn\n",
        "scorefunction(label_1,y_test)\n",
        "scorefunction(label_2,y_test)\n",
        "scorefunction(label_3,y_test)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj-UFnL4NteJ"
      },
      "source": [
        "columns=['Model_Name','Precision','Recall','Macro_F1-score']\n",
        "performace = pd.DataFrame()\n",
        "performace['Model_Name'] = Modelnames\n",
        "performace['Precision'] = precision1\n",
        "performace['Recall'] = recall1\n",
        "performace['Macro_F1-score'] = f1Score1"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8_i2aTFiB2q"
      },
      "source": [
        "5. Add to the comparison of #4 a the manually calculated precision, recall and F1 score using VADER and their suggested defaults to categorize the test split tweets in positive or negative. Answer the following questions: Is this approach as good as the previous ones? Why do you think this is? (30 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "w_rT_lPSPhRk",
        "outputId": "1c0758c9-7a65-4c8b-999e-6b1620872b0f"
      },
      "source": [
        "import nltk \n",
        "import numpy\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLspmHXhYQUs"
      },
      "source": [
        "def VADER_STUFF(x_test):\n",
        "\n",
        "    new_copy = pd.DataFrame(x_test,columns=[\"text\"])\n",
        "    compound = []\n",
        "\n",
        "    for i in new_copy['text']:\n",
        "      generated_array = SentimentIntensityAnalyzer().polarity_scores(i)\n",
        "      compound.append(generated_array[\"compound\"])\n",
        "\n",
        "    new_copy[\"compound\"] = compound\n",
        "\n",
        "    value = []\n",
        "    for k in new_copy[\"compound\"]:\n",
        "      if row >= 0.05:\n",
        "          value.append(\"1\")\n",
        "      else:\n",
        "        value.append(\"0\")"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09kxecnRmqGt"
      },
      "source": [
        "VADER_STUFF(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOeIuFsXd2nW"
      },
      "source": [
        "Vad_pred = new_copy.value\n",
        "\n",
        "accuracy5 = accuracy_score(y_test,Vad_pred)\n",
        "recall5 = recall_score(y_test,Vad_pred)\n",
        "precision5 = precision_score(y_test,Vad_pred)\n",
        "f1 =() 2 * (precision5*recall5)) /(precision5+recall5)\n",
        "\n",
        "Print(\"Using VADER our Accuracy score is \"+accuracy5)\n",
        "Print(\"Recall is \"+recall5)\n",
        "Prrint(\"Precison is \"+precision5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5OV9ov-iOMp"
      },
      "source": [
        "Print(\"Using VADER our Accuracy score is \"+accuracy5)\n",
        "Print(\"Recall is \"+recall5)\n",
        "Prrint(\"Precison is \"+precision5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFNe3-7niFHv"
      },
      "source": [
        "Bonus (30 points): Try the following things to improve the LSTM model:\n",
        "\n",
        "Answer the following questions: Did this change the results in any way? Why do you think so?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1woaqO_cDtw"
      },
      "source": [
        "**THESE DIDNT COMPILE ON TIME.PLEASE CHECK MY GITHUB FOR THE OUTPUT CELLS.THANK YOU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoAI8fMLcdwB"
      },
      "source": [
        "[MY GITHUB PAGE](https://https://github.com/Kausik-A/CSC-8980-Natural-Language-Processing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U56rIzc_0cn9"
      },
      "source": [
        "1) Use 90% training data, 10% testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5TTaLO3WWS9"
      },
      "source": [
        "x_train_2,x_test_2,y_train_2,y_test_2 = train_test_split(PN_Clean.text,PN_Clean.Polarity,test_size=0.2,random_state=2361)\n",
        "LSTM_Model_2 = LSTM_Classify(PN_Clean,x_train_2,x_test_2,y_train_2,y_test_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrrzgh6Q0ceT"
      },
      "source": [
        "2) Remove stopwords from the tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etgUaCUxWb0d"
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords  \n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "new_copy_5 = PN_Clean.copy()\n",
        "\n",
        "LSTM_Model_2 = LSTM_Classify(new_copy_5,x_train_2,x_test_2,y_train_2,y_test_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-o0vEqW0cTB"
      },
      "source": [
        "3) Remove all user mentions for the tweets (@something)\n",
        "Compare all three new models in terms of their precision, recall and F1 score. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYUQ9QMchxuS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}