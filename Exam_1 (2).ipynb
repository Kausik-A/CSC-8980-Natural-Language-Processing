{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exam_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i7cYcpmQ-Ub"
      },
      "source": [
        "\n",
        "**Name: Kausik Amancherla**\n",
        "\n",
        "**Student ID: 002544017**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "O1uGCMs9ITbJ",
        "outputId": "b87f443c-c05f-4472-9ff2-e1e0825ce57b"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name=\"/content/exam1_dataset.zip\"\n",
        "\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "  zip.extractall(\"/content/\")\n",
        "  print('Done')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWKLDe0mlo-C"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import os, random,io\n",
        "import numpy as np\n",
        "import nltk\n",
        "#import matplotlib import pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm5rPXgHwcNR"
      },
      "source": [
        "path_1 = '/content/exam1_dataset/TRAINING/positive'\n",
        "path_2 = '/content/exam1_dataset/TRAINING/negative'\n",
        "txt_1 = os.listdir(path_1)\n",
        "txt_2 = os.listdir(path_2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q6XHSae7Vfu"
      },
      "source": [
        "positive = []\n",
        "negative = []\n",
        "mix = []\n",
        "\n",
        "for f in txt_1 :\n",
        "  positive.append(io.open(os.path.join(path_1,f), 'r').read())\n",
        "\n",
        "for f in txt_2 :\n",
        "  negative.append(io.open(os.path.join(path_2,f), 'r').read())\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk4ULndo2ncB"
      },
      "source": [
        "#mix.append(positive)\n",
        "#mix.append(negative)\n",
        "mix = positive+negative"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFv-XRHn5jJp"
      },
      "source": [
        "dataframe_positive = pd.DataFrame(positive,columns =['data'])\n",
        "dataframe_positive['label'] = 'Positive'\n",
        "\n",
        "dataframe_negative = pd.DataFrame(negative,columns =['data'])\n",
        "dataframe_negative['label'] = 'Negative'\n",
        "\n",
        "dataframe_mix = pd.concat([dataframe_positive,dataframe_negative])\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCWu20KqQ-QY"
      },
      "source": [
        "**Question 1)​ ​(20 points)**​ Write a​ generic function​ that takes: Classification algorithm name, vectorization method name, training set with labels as parameters (total of 3 parameters should be passed). The function should take the classification algorithm name, the vectorization method’s name, and the training set and train the desired model. Use the default training parameters for the models we have seen in class. This function should return the trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLzpY4_Q89X7"
      },
      "source": [
        "def Model_Creation(Algorithm,Vectorization,Training):\n",
        "   model = make_pipeline(Vectorization(),Algorithm()) \n",
        "   model = model.fit(Training.data,Training.label)\n",
        "   return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVJeo528Q-Mi"
      },
      "source": [
        "**Question 2)​ ​(30 points)​** Using the function from question 1 to ​build the following models​:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSUsNHJu-Oiv"
      },
      "source": [
        "a) Model a: Naive Bayes, Vectorizer: TFIDF and Bag of Words, Training set should be 75%\n",
        "of the provided dataset. Leaving the remaining 25% for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyEKkhCS-kw7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "#Splitting the data into training and testing\n",
        "Naive_Training_data,Naive_Testing_data = train_test_split(dataframe_mix,test_size=0.25,random_state=12345)\n",
        "\n",
        "# TFIDF model training\n",
        "NV_model_1=Model_Creation(MultinomialNB,TfidfVectorizer,Naive_Training_data)\n",
        "\n",
        "# Bag of Words training\n",
        "NV_model_2=Model_Creation(MultinomialNB,CountVectorizer,Naive_Training_data)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zXZrSk5-OZJ"
      },
      "source": [
        "b) Model b: RandomForest, Vectorizer: TFIDF and Bag of Words, Training set should be\n",
        "70% of the provided dataset. Leaving the remaining 30% for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0hYvTFbFVgN"
      },
      "source": [
        "#Splitting the data into training and testing\n",
        "RandomForest_Training_data,RandomForest_Testing_data = train_test_split(dataframe_mix,test_size=0.30,random_state=12345)\n",
        "\n",
        "# TFIDF model training\n",
        "RF_model_1=Model_Creation(RandomForestClassifier,TfidfVectorizer,RandomForest_Training_data)\n",
        "\n",
        "# Bag of Words training\n",
        "RF_model_2=Model_Creation(RandomForestClassifier,CountVectorizer,RandomForest_Training_data)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U2zDnw--OJB"
      },
      "source": [
        "c) Model c: Support Vector Machines (SVC in sklearn), Vectorizer: TFIDF and Bag of\n",
        "Words, Training set should be 60% of the provided dataset. Leaving the remaining 40% for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWwkMDC5GGP6"
      },
      "source": [
        "#Splitting the data into training and testing\n",
        "SVM_Training_data,SVM_Testing_data = train_test_split(dataframe_mix,test_size=0.40,random_state=12345)\n",
        "\n",
        "# TFIDF model training\n",
        "SVM_model_1=Model_Creation(SVC,TfidfVectorizer,SVM_Training_data)\n",
        "\n",
        "# Bag of Words training\n",
        "SVM_model_2=Model_Creation(SVC,CountVectorizer,SVM_Training_data)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlZElkh5Q-Iz"
      },
      "source": [
        "**Question 3)​ ​(30 points)**​ Using the models from Question 2, evaluate each model with its respective training set (for ​model a,​ that set is 25% of the data, for ​model b​, that set is 30% of the data, and for ​model c​ that set is 40% of the data. Be careful to not mix up the evaluation sets. With the predictions on the test set and show the following metrics: Accuracy, Precision, Recall, and Macro F1-score. With this in mind, please write and answer these questions in your notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLHRg2zgY7jG"
      },
      "source": [
        "label_1=NV_model_1.predict(Naive_Testing_data.data)\n",
        "label_2=NV_model_2.predict(Naive_Testing_data.data)\n",
        "label_3=RF_model_1.predict(RandomForest_Testing_data.data)\n",
        "label_4=RF_model_2.predict(RandomForest_Testing_data.data)\n",
        "label_5=SVM_model_1.predict(SVM_Testing_data.data)\n",
        "label_6=SVM_model_2.predict(SVM_Testing_data.data)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDEN0hcvaaAq"
      },
      "source": [
        "import sklearn\n",
        "\n",
        "f1Score1 = []\n",
        "accuracy1 = []\n",
        "precision1 = []\n",
        "recall1 = []\n",
        "\n",
        "columns=['Model_Name','Vectorizer','Accuracy','Precision','Recall','Macro_F1-score']\n",
        "Modelnames = ['Naive_Bayes','Naive_Bayes','RandomForest','RandomForest','Support_Vector_Machines','Support_Vector_Machines']\n",
        "vec = ['TFIDF','Bag_of_Words','TFIDF','Bag_of_Words','TFIDF','Bag_of_Words']\n",
        "train = [75,75,70,70,60,60]\n",
        "test = [25,25,30,30,40,40]\n",
        "\n",
        "\n",
        "\n",
        "#performace = performace.append(names,ignore_index=True)\n",
        "\n",
        "def scorefunction(label,testdata):\n",
        "  temp= sklearn.metrics.f1_score(label,testdata.label,average='macro')\n",
        "  temp2 = sklearn.metrics.accuracy_score(testdata.label,label)\n",
        "  temp3 = sklearn.metrics.precision_score(testdata.label,label,pos_label = \"Positive\")\n",
        "  temp4 = sklearn.metrics.recall_score(testdata.label,label,pos_label = \"Positive\")\n",
        "\n",
        "  #Lt=pd.DataFrame([listtemp],columns=['Model_Name','Vectorizer','Accuracy','Precision','Recall','Macro_F1-score'])\n",
        "  #performace = pd.DataFrame(listtemp, columns=['Model_Name','Vectorizer','Accuracy','Precision','Recall','Macro_F1-score'])\n",
        "  #print(Lt)\n",
        "  #performace=pd.DataFrame(listtemp)\n",
        "  \n",
        "  f1Score1.append(temp)\n",
        "  accuracy1.append(temp2)\n",
        "  precision1.append(temp3)\n",
        "  recall1.append(temp4)\n",
        "  "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEy1mol-gigx"
      },
      "source": [
        "scorefunction(label_1,Naive_Testing_data)\n",
        "scorefunction(label_2,Naive_Testing_data)\n",
        "scorefunction(label_3,RandomForest_Testing_data)\n",
        "scorefunction(label_4,RandomForest_Testing_data)\n",
        "scorefunction(label_5,SVM_Testing_data)\n",
        "scorefunction(label_6,SVM_Testing_data)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfdszFNn68Pc"
      },
      "source": [
        "columns=['Model_Name','Vectorizer','Accuracy','Precision','Recall','Macro_F1-score']\n",
        "performace = pd.DataFrame()\n",
        "performace['Model_Name'] = Modelnames\n",
        "performace['Training %'] = train\n",
        "performace['Test %'] = test\n",
        "performace['Vectorizer'] = vec\n",
        "performace['Accuracy'] = accuracy1\n",
        "performace['Precision'] = precision1\n",
        "performace['Recall'] = recall1\n",
        "performace['Macro_F1-score'] = f1Score1\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIChdyQrQ-C5"
      },
      "source": [
        "a) What model performs the best and why? (which metrics do you base this on, and why do you think it performs better than others)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNdrbOgkTx9V"
      },
      "source": [
        "**Best Model :** Support Vector Machines with TFIDF Vectorizer\n",
        "\n",
        "**Metric:** Accuracy\n",
        "\n",
        "**Why:** Both positive and negative training datasets are equally distrubuted around ~12500 files each.As they are balanced, accuracy is a correct metric in comparing which model is the best.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5wBo3hNQ968"
      },
      "source": [
        "b) Why is it important not to mix up the testing sets between different models? Think about this one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYvHu4tPVPKV"
      },
      "source": [
        "Every model was specififed a different percentage of Training data and Test data.So the split affect the way a model has been trained. There should be balance so that it does not result in over-fitting or under-fitting. We do this cause by using similar data for training and testing, we can minimize the effects of data discrepancies and better understand the characteristics of the model. Thus we cant use testing sets between different models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKsd244mQ9wC"
      },
      "source": [
        "c) Display in a single sorted dataframe (model name, training %, test %, accuracy, precision, recall, F1-score) all performance metrics, sorted by accuracy in descending manner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxwmFmhk9RXT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "56bc4a4f-d1f3-4d4b-df51-850d02fdfd7f"
      },
      "source": [
        "performace.sort_values(by = 'Accuracy',ascending=False)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model_Name</th>\n",
              "      <th>Training %</th>\n",
              "      <th>Test %</th>\n",
              "      <th>Vectorizer</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Macro_F1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Support_Vector_Machines</td>\n",
              "      <td>60</td>\n",
              "      <td>40</td>\n",
              "      <td>TFIDF</td>\n",
              "      <td>0.892200</td>\n",
              "      <td>0.883411</td>\n",
              "      <td>0.905829</td>\n",
              "      <td>0.892150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Naive_Bayes</td>\n",
              "      <td>75</td>\n",
              "      <td>25</td>\n",
              "      <td>TFIDF</td>\n",
              "      <td>0.862080</td>\n",
              "      <td>0.895093</td>\n",
              "      <td>0.827338</td>\n",
              "      <td>0.862046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Support_Vector_Machines</td>\n",
              "      <td>60</td>\n",
              "      <td>40</td>\n",
              "      <td>Bag_of_Words</td>\n",
              "      <td>0.853800</td>\n",
              "      <td>0.842578</td>\n",
              "      <td>0.873315</td>\n",
              "      <td>0.853692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Naive_Bayes</td>\n",
              "      <td>75</td>\n",
              "      <td>25</td>\n",
              "      <td>Bag_of_Words</td>\n",
              "      <td>0.851360</td>\n",
              "      <td>0.882333</td>\n",
              "      <td>0.818580</td>\n",
              "      <td>0.851331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>70</td>\n",
              "      <td>30</td>\n",
              "      <td>Bag_of_Words</td>\n",
              "      <td>0.845333</td>\n",
              "      <td>0.852832</td>\n",
              "      <td>0.842133</td>\n",
              "      <td>0.845304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>70</td>\n",
              "      <td>30</td>\n",
              "      <td>TFIDF</td>\n",
              "      <td>0.836800</td>\n",
              "      <td>0.858205</td>\n",
              "      <td>0.814689</td>\n",
              "      <td>0.836795</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Model_Name  Training %  ...    Recall Macro_F1-score\n",
              "4  Support_Vector_Machines          60  ...  0.905829       0.892150\n",
              "0              Naive_Bayes          75  ...  0.827338       0.862046\n",
              "5  Support_Vector_Machines          60  ...  0.873315       0.853692\n",
              "1              Naive_Bayes          75  ...  0.818580       0.851331\n",
              "3             RandomForest          70  ...  0.842133       0.845304\n",
              "2             RandomForest          70  ...  0.814689       0.836795\n",
              "\n",
              "[6 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUl33341Q9fn"
      },
      "source": [
        "**Question 4)​ ​(15 points)**​ Using the documents in the folder named UNLABELED, please use your best performing trained model from question 3 to predict their labels. Please do this individually for each document. ​Print to the screen the following items: Document Name, Predicted Label and using a text cell, write your own opinion if the label is correct and why - note you have to read the document to make your own opinion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNnkcWGO_gHe"
      },
      "source": [
        "path_3 = '/content/exam1_dataset/UNLABELED/'\n",
        "txt_3 = os.listdir(path_3)\n",
        "\n",
        "unlabel2 = []\n",
        "\n",
        "#for f in txt_3 :\n",
        "  #unlabel.append(io.open(os.path.join(path_3,f), 'r').read())\n",
        "\n",
        "#for i in len(unlabel)\n",
        "  #label_7=SVM_model_1.predict([unlabel[i]])\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKaQwvE4JUb6"
      },
      "source": [
        "def unlabelcheck(passfile):\n",
        "        unlabel2 = (open(path_3+passfile, 'r').read())\n",
        "        givenprediction=SVM_model_1.predict([unlabel2])\n",
        "        print('File Name:', passfile)\n",
        "        print('Predicited Label:', givenprediction)\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELjmgIzaMez6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "10dc9364-02c8-495b-a043-843fd09ee727"
      },
      "source": [
        "file1=txt_3[0]\n",
        "unlabelcheck(file1)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name: 46278_0.txt\n",
            "Predicited Label: ['Negative']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8MvQ-EK0L3q"
      },
      "source": [
        "**Actual Label:** Positive\n",
        "\n",
        "**Opinion:** This was a tough one to predict as the review is a personal experience of what was going on with the person.It didnt contain any opinion about the mobie so the whole testing data in the file is not inline to be used for an accurate prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EnghVcMTlFR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "be0fbf5f-e305-4874-ecd6-d7fef193ca16"
      },
      "source": [
        "file2=txt_3[1]\n",
        "unlabelcheck(file2)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name: 36517_0.txt\n",
            "Predicited Label: ['Negative']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L16_adLX2gP3"
      },
      "source": [
        "\n",
        "**Actual Label:** Negative\n",
        "\n",
        "**Opinion:** The review has many negative verbs and nouns such as 'bad','worst','terrible','nasty'.All these words are a good indecator that the review is bad.The model catches these words and thus accurately predicts the label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ1AeJ96Tk9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e95671fb-60dd-45b9-af97-7d5fb8e90b2e"
      },
      "source": [
        "file3=txt_3[2]\n",
        "unlabelcheck(file3)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name: 24221_0.txt\n",
            "Predicited Label: ['Positive']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaaP5Cwn3XC4"
      },
      "source": [
        "**Actual Label:** Negative\n",
        "\n",
        "**Opinion:** Eventhough the review(testdata) is long and the first part of it does not relate to actual review,It does contain enough negative verbs and nouns such as 'bad','boring','terrible','strange'.All these words are a good indicator that the review is bad.The model catches these words and thus accurately predicts the label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x5cBKKgTk28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8bf98b8e-3907-4b53-810a-22b4132fb3d8"
      },
      "source": [
        "file4=txt_3[3]\n",
        "unlabelcheck(file4)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name: 46705_0.txt\n",
            "Predicited Label: ['Negative']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jdb6EJr4IaJ"
      },
      "source": [
        "**Actual Label:** Negative\n",
        "\n",
        "**Opinion:** Eventhough the review(testdata) is long and most part of it does not relate to actual review,It does contain enough negative verbs and nouns such as 'worst','boring','terrible','crap'.All these words are a good indicator that the review is bad.The model catches these words and thus accurately predicts the label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibH8fGhkTkvp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ca73339b-4e91-428c-9943-b171812d7cf8"
      },
      "source": [
        "file5=txt_3[4]\n",
        "unlabelcheck(file5)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name: 37154_0.txt\n",
            "Predicited Label: ['Negative']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WOYfPDn4nxv"
      },
      "source": [
        "**Actual Label:** Negative\n",
        "\n",
        "**Opinion:** The review has many negative verbs and nouns such as 'lame','bad','boredom',etc.All these words are a good indecator that the review is bad.The model catches these words and thus accurately predicts the label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_RZEZHJTknu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "16ef2272-77d7-4da9-d37d-b0516d37f3a8"
      },
      "source": [
        "file6=txt_3[5]\n",
        "unlabelcheck(file6)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name: 0_0.txt\n",
            "Predicited Label: ['Positive']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF9WFFTL5H_z"
      },
      "source": [
        "**Actual Label:** Negative\n",
        "\n",
        "**Opinion:** The review had positive words such as 'good','great',etc but they were not used in the contect of the movie review.This is a tough one to predict cause the model was fundametally trained on picking the positive/negative rather rather than understanding the whole context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfMkZ8o9Tket",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5b84fc72-a630-4e3f-cfaa-0aa6820261d2"
      },
      "source": [
        "file7=txt_3[6]\n",
        "unlabelcheck(file7)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name: 36022_0.txt\n",
            "Predicited Label: ['Negative']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq_xh86E6wOt"
      },
      "source": [
        "**Actual Label:** Negative\n",
        "\n",
        "**Opinion:** The review is short so there is not much data to evaluate with.But as it does not contain and positive words and is very monotomous,the model picks negative by evaluating the words such as 'better'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9q3oyJ8TkIk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "46d54bcd-5a63-40dc-bdbe-b2b01ca80699"
      },
      "source": [
        "file8=txt_3[7]\n",
        "unlabelcheck(file8)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name: 36149_0.txt\n",
            "Predicited Label: ['Negative']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3QNqYTU7Pzy"
      },
      "source": [
        "**Actual Label:** Negative\n",
        "\n",
        "**Opinion:** The review is short so there is not much data to evaluate with.But as it does not contain and positive words,the model picks negative by evaluating the words such as 'bad','naseous','better' as they generally keep appearing in bad reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyr300lwTYe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "18e5e330-4e6f-4ebe-ce05-c9854cb2f379"
      },
      "source": [
        "file9=txt_3[8]\n",
        "unlabelcheck(file9)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name: 35991_0.txt\n",
            "Predicited Label: ['Negative']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp0Hoe0y7zmV"
      },
      "source": [
        "**Actual Label:** Negative\n",
        "\n",
        "**Opinion:** The review is short so there is not much data to evaluate with.But as it does not contain and positive words,the model picks negative by evaluating the words such as 'bad','naseous','better' as they generally keep appearing in bad reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btpv0nKbT1bn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8746b879-914e-42e3-92df-97bdb32a59de"
      },
      "source": [
        "file10=txt_3[9]\n",
        "unlabelcheck(file10)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name: 49990_0.txt\n",
            "Predicited Label: ['Negative']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JWCrF8F7646"
      },
      "source": [
        "**Actual Label:** Negative\n",
        "\n",
        "**Opinion:** The review has many negative verbs and nouns such as 'lame','bad','boredom','uninvolving','agonizingly'.All these words are a good indecator that the review is bad.The model catches these words and thus accurately predicts the label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB297_SOT2EP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "cc0174a1-b7f6-4a83-f421-86dca8651447"
      },
      "source": [
        "file11=txt_3[10]\n",
        "unlabelcheck(file11)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name: 35968_0.txt\n",
            "Predicited Label: ['Positive']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB5eSbPs8anP"
      },
      "source": [
        "**Actual Label:** Negative\n",
        "\n",
        "**Opinion:** The review had positive words such as 'good','great',etc but they were not used in the contect of the movie review.This is a tough one to predict cause the model was fundametally trained on picking the positive/negative rather rather than understanding the whole context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOK8Rq5ASDKw"
      },
      "source": [
        "**Question 5)​** ​(20 points)​ Build a function that takes the set of documents as input and returns a cosine similarity matrix for those documents. Feed all documents in the TRAINING folder to this matrix. Instead of printing the returned cosine similarity matrix, create a heatmap plot from the returned matrix. ​Make sure y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kuSpHQrJld-"
      },
      "source": [
        "tfidf = TfidfVectorizer()\n",
        "#matrix = tfidf.fit_transform(dataframe_mix[\"data\"].values.astype('U'))\n",
        "matrix = tfidf.fit_transform(mix)"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuAtLuZHy280"
      },
      "source": [
        "#batch=300\n",
        "batch=400\n",
        "#batch=500\n",
        "\n",
        "Similarity_Matrix = np.ndarray((matrix.shape[0], matrix.shape[0]))\n",
        "#print(matrix.shape[0])\n",
        "\n",
        "batch_split=matrix.shape[0] / batch    \n",
        "for j in range(0, int(batch_split) + 1):\n",
        "     x = j * batch\n",
        "     #y = min([(j + 1) * batch, matrix.shape[0]])\n",
        "     boop = (j + 1) * batch\n",
        "     beep = matrix.shape[0]\n",
        "     if beep<boop:\n",
        "       y = beep\n",
        "     else:\n",
        "       y = boop\n",
        "     if y <= x:\n",
        "        break \n",
        "     split1 = matrix[x: y]\n",
        "     #print(split1)\n",
        "     sim = cosine_similarity(split1, matrix)\n",
        "     #print(sim) \n",
        "     Similarity_Matrix[x: y] = sim"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6UbCQX3np7l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3ea92586-0f49-4f31-d8d6-741b69f94683"
      },
      "source": [
        "Similarity_Matrix"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.09456968, 0.15695999, ..., 0.10349039, 0.11015603,\n",
              "        0.12704311],\n",
              "       [0.09456968, 1.        , 0.13686362, ..., 0.08375349, 0.13037102,\n",
              "        0.1116729 ],\n",
              "       [0.15695999, 0.13686362, 1.        , ..., 0.17148117, 0.18513416,\n",
              "        0.1666143 ],\n",
              "       ...,\n",
              "       [0.10349039, 0.08375349, 0.17148117, ..., 1.        , 0.16533383,\n",
              "        0.12391067],\n",
              "       [0.11015603, 0.13037102, 0.18513416, ..., 0.16533383, 1.        ,\n",
              "        0.14088131],\n",
              "       [0.12704311, 0.1116729 , 0.1666143 , ..., 0.12391067, 0.14088131,\n",
              "        1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrc7_1SWoRGy"
      },
      "source": [
        "\n",
        "######## RUN IT AT OWN RISK!!!!! ###########\n",
        "####### THIS RESULTS IN MEMORY OVERLOAD AND CRASHES THE KERNAL #######\n",
        "######## If this notebook is running on a local system with more than 32gb of ram then the below code can be commented out *fingers crossed * ########\n",
        "\n",
        "\n",
        "#sns.heatmap(Similarity_Matrix.T)\n",
        "#plt.xlabel('true label')\n",
        "#plt.ylabel('predicted label');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2veLzrq5SJkM"
      },
      "source": [
        "**Question 6)​ ​(15 points)**​ Write a function that takes a cosine similarity matrix as input and returns a list with the top n document paris and their similarity. Note that you should only keep the document pairs that are unique and remove the comparisons of the document to itself. Print the top 50 similar document pairs. Compare the assigned class for each document and answer: Do all similar documents belong to the same class? Why or why not?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCtcHvCV1_GY"
      },
      "source": [
        "# My homework approach just crashes the entire notebook so not going to mess with this\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe3FbCC0SMpw"
      },
      "source": [
        "**Question 7)​ ​(20 points)**​ Using Spacy’s part of speech tagger, process all sentences (hint: don’t forget to split the reviews) and count how many NOUN and VERB tags are found in all the movies review (TRAINING folder) separating them by label. In other words, how many NOUN and VERB tags are found in positive reviews, and how many NOUN and VERB tags are found in negative reviews. ​Answer the following question: When comparing both, do you see any differences? Why do you think about the differences? Or lack of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7sB3ODhZ8ar"
      },
      "source": [
        "#### Takes around 4-5 Min to run completely,grab a coffee\n",
        "import spacy\n",
        "\n",
        "count=0 #noun\n",
        "count2=0 #verb\n",
        "count3=0 #punct\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "idonthavesleep=[] \n",
        "\n",
        "for f in range(len(positive)) :\n",
        "  #idonthavesleep = (io.open(os.path.join(path_1,f), 'r').read())\n",
        "  \n",
        "    doc = nlp(positive[f])\n",
        "    for token in doc:\n",
        "        #print(token.text,token.pos_)\n",
        "        temp=token.pos_\n",
        "        if temp == 'NOUN':\n",
        "          count=count+1\n",
        "        elif temp == 'VERB':\n",
        "          count2=count2+1\n",
        "        elif temp == 'PUNCT':\n",
        "          count3=count3+1\n",
        "          \n",
        "    #print(f) ( Can uncomment this to keep track of the progress. f from 1 till 25000)   \n",
        "\n",
        "#print(count)\n",
        "#print(count2)\n",
        "#print(count3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kRNub-XDjy5u",
        "outputId": "64f294ad-c1d7-4e67-afc9-37fd3f56d93f"
      },
      "source": [
        "print('Total Number of NOUNS in Positive Dataset:',count)\n",
        "print('Total Number of VERBS in Positive Dataset:',count2)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Number of NOUNS in Positive Dataset: 1071453\n",
            "Total Number of VERBS in Positive Dataset: 697684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyFv-vvGknbw"
      },
      "source": [
        "### this if for negative dataset####\n",
        "count4=0 #noun\n",
        "count5=0 #verb\n",
        "count6=0 #punct\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "for f in range(len(negative)) :\n",
        "  #idonthavesleep = (io.open(os.path.join(path_1,f), 'r').read())\n",
        "  \n",
        "    doc = nlp(negative[f])\n",
        "    for token in doc:\n",
        "        #print(token.text,token.pos_)\n",
        "        temp=token.pos_\n",
        "        if temp == 'NOUN':\n",
        "          count4=count4+1\n",
        "        elif temp == 'VERB':\n",
        "          count5=count5+1\n",
        "        elif temp == 'PUNCT':\n",
        "          count6=count6+1\n",
        "          \n",
        "    #print(f) ( Can uncomment this to keep track of the progress, f from 1 till 25000 )     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cAqAU_HGpJoe",
        "outputId": "5952360c-00ca-400e-fbc7-525b3444cc9d"
      },
      "source": [
        "print('Total Number of NOUNS in Negative Dataset',count4)\n",
        "print('Total Number of VERBS in Negative Dataset',count5)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Number of NOUNS in Negative Dataset 528475\n",
            "Total Number of VERBS in Negative Dataset 355559\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_pdWlJESca3"
      },
      "source": [
        "**Question 8) (20 points)** Using the results from the PoS process in question 7, count how many\n",
        "different PUNCT tags are found and their respective counts from the full dataset provided (both\n",
        "negative and positives together). Using regex, write a set of regular expressions that generate\n",
        "the same counts from the dataset without using NLTK or Spacy, just regex. Can you get the\n",
        "same counts? If not, why do you think this is?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHWlmq_vjDEf"
      },
      "source": [
        "import re\n",
        "\n",
        "string = []\n",
        "puncount = 0\n",
        "for f in range(len(mix)):\n",
        "    #Referenced it from Spacey Puntuation.py Documentation \n",
        "    result = re.findall(r'[!\"\\$%&\\'()*+,\\-.\\/:;=#@?\\[\\\\\\]^_`{|}~]*',mix[f])\n",
        "    string = \"\".join(result)\n",
        "    puncount = puncount + len(string)\n",
        "\n",
        "#print(puncount)\n",
        "#print(string)\n",
        "#print(len(string))"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "edHSj90eivtC",
        "outputId": "d1f878a3-3127-4947-926d-ce05ad8b3774"
      },
      "source": [
        "punct = count3+count6\n",
        "print('The number of PUNCT using Spacy is',punct)\n",
        "print('The number of PUNCT using RegEx is',puncount)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of PUNCT using Spacy is 1252859\n",
            "The number of PUNCT using RegEx is 1123492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s54xWUv-xew"
      },
      "source": [
        "Looks like both Spacy and RegEx dont give exactly the same count value.By my guess i think it is because of the difference in the fundamental principals of both of these. In RegEx we write the rules, the way we want. We tweak it as per our requirment. But Spacy follows 'Rule based Matching'.So it is predefined.\n",
        "Thus there are some differences in deciding which characters might be puntuations as in regEx we have to hardcode and let the program know what they are. This is not the case in Spacy as it is pre-defined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqi0ETzgSgr1"
      },
      "source": [
        "**Bonus Question:​ ​(40 points)**​ Using the code from Class 09 - Word Embeddings, pre-tune BERT in order to classify movie reviews. You can use the full TRAINING folder for the tuning and use the UNLABELED folder for your final classification/prediction task. Do the label predictions from BERT match what your classifier from Question 4 predicted? If they don’t, are they better? Please say why."
      ]
    }
  ]
}