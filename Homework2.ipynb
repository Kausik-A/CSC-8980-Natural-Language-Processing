{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci5Aws0g6t7W"
      },
      "source": [
        "\n",
        "**Name: Kausik Amancherla**\n",
        "\n",
        "**Student ID: 002544017**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_ES0SUe7fON"
      },
      "source": [
        "1) Create a cosine similarity matrix for all of Shakespeare’s works found in the provided \u0000le. This will result in a 42 by 42 matrix with the cosine similarity between each of his works. In other words, calculate the document-wise cosine similarity between all of Shakepeare’s works. Use TF_IDF for this. Note, you can use the Cosine Similarity function on scikit-learn or implement your own, but no other library/package is allowed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWKLDe0mlo-C",
        "outputId": "38ca0612-26a4-423a-a340-a8ed897ab35e"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import os, random\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vpC8CCV7Or5"
      },
      "source": [
        "path = '/content/drive/MyDrive/shakespeares-works_TXT_FolgerShakespeare'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTmkg8H97UoB"
      },
      "source": [
        "txt = os.listdir(path)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q6XHSae7Vfu"
      },
      "source": [
        "data = []\n",
        "for f in txt :\n",
        "  data.append(open(os.path.join(path,f), 'r').read())\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "matrix = tfidf.fit_transform(data)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT4m1ytb774z",
        "outputId": "3b603218-8b14-4afd-dac3-f6533ffb6a1c"
      },
      "source": [
        "Similarity_Matrix = cosine_similarity(matrix,matrix)\n",
        "print(Similarity_Matrix)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.6106152  0.58915758 ... 0.59843788 0.71367332 0.69815654]\n",
            " [0.6106152  1.         0.49961181 ... 0.49330278 0.58762884 0.57264454]\n",
            " [0.58915758 0.49961181 1.         ... 0.47403783 0.5709058  0.55048388]\n",
            " ...\n",
            " [0.59843788 0.49330278 0.47403783 ... 1.         0.56273688 0.55511345]\n",
            " [0.71367332 0.58762884 0.5709058  ... 0.56273688 1.         0.66390227]\n",
            " [0.69815654 0.57264454 0.55048388 ... 0.55511345 0.66390227 1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnHsG5zc-Pgh"
      },
      "source": [
        "2) Write a function that takes the previous matrix and a number n as parameters (nothing else will be accepted) and return the top n similar works. Use the function to output the top 10 similar works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN7mlRPo-YAc"
      },
      "source": [
        "def best_works(Similarity_Matrix, N):\n",
        "  for i in range(len(Similarity_Matrix)):\n",
        "    Similarity_Matrix[i][i] = 0 \n",
        "\n",
        "  one_dimension = Similarity_Matrix.flatten().argsort()[-(2*N):]\n",
        "  xrow, yrow = np.unravel_index(one_dimension, Similarity_Matrix.shape)\n",
        "  xrow = np.flip(xrow)\n",
        "  \n",
        "  yrow = np.flip(yrow)\n",
        "  top_10 = []\n",
        "\n",
        "  for i in range(0,len(xrow),2):\n",
        "    x = xrow[i]\n",
        "    y = yrow[i]\n",
        "    if(x != y):\n",
        "      top_10.append((x,y,Similarity_Matrix[x][y]))\n",
        "  return top_10"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNRtGhLZ-fGx"
      },
      "source": [
        "top_10 = best_works(Similarity_Matrix,10)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSU3yplg-fyI",
        "outputId": "4d773d23-b05a-48e9-bc74-d671084eacdc"
      },
      "source": [
        "print(top_10)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(7, 13, 0.9175430138884018), (6, 22, 0.9076165568984125), (35, 7, 0.8677688748875237), (25, 5, 0.8437915980298399), (16, 18, 0.8389525010107515), (13, 35, 0.838145210826978), (38, 4, 0.8363042342905279), (25, 15, 0.835492667018653), (25, 16, 0.8343507770652856), (4, 0, 0.8342954531999507)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPlZeKvL-wEZ"
      },
      "source": [
        "3) Using the code from the Language Models II class, train two simple language models using all of the less (together) in shakespeares-works_TXT_FolgerShakespeare.zip. One model should be trained using bigrams, the other using trigrams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snw0dx7P-voK"
      },
      "source": [
        "tokens = []\n",
        "for doc in data:\n",
        "  tokens.append(nltk.word_tokenize(doc))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSjCljU4_NZq"
      },
      "source": [
        "We are going to use Bigrams :\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2XzYzR9-3nI"
      },
      "source": [
        "from nltk.corpus import reuters\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JomD3zo_Qhp"
      },
      "source": [
        "bigram_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "for sentence in tokens:\n",
        "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
        "        bigram_model[w1][w2] += 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taHxTOWk_YlA"
      },
      "source": [
        "for w1 in bigram_model:\n",
        "    total_count = float(sum(bigram_model[w1].values()))\n",
        "    for w2 in bigram_model[w1]:\n",
        "        bigram_model[w1][w2] /= total_count"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPSeWf-v_mC0",
        "outputId": "4e4f2e1f-40ac-4fd6-f2f9-488ffd3f6353"
      },
      "source": [
        "dict(bigram_model[\"King\"])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!': 0.011428571428571429,\n",
              " \"''\": 0.0016326530612244899,\n",
              " \"'s\": 0.09551020408163265,\n",
              " ',': 0.11836734693877551,\n",
              " '--': 0.0024489795918367346,\n",
              " '.': 0.08897959183673469,\n",
              " ':': 0.004081632653061225,\n",
              " ';': 0.004081632653061225,\n",
              " '?': 0.0236734693877551,\n",
              " 'And': 0.006530612244897959,\n",
              " 'Be': 0.0008163265306122449,\n",
              " 'Bolingbroke': 0.0008163265306122449,\n",
              " 'Brothers': 0.0008163265306122449,\n",
              " 'Cambyses': 0.0008163265306122449,\n",
              " 'Capaneus': 0.0008163265306122449,\n",
              " 'Cerberus': 0.0008163265306122449,\n",
              " 'Claudius': 0.0008163265306122449,\n",
              " 'Clothair': 0.0008163265306122449,\n",
              " 'Cophetua': 0.0024489795918367346,\n",
              " 'Digest': 0.0008163265306122449,\n",
              " 'Dismiss': 0.0008163265306122449,\n",
              " 'Does': 0.0008163265306122449,\n",
              " 'Duncan': 0.0024489795918367346,\n",
              " 'Edward': 0.027755102040816326,\n",
              " 'Five': 0.0008163265306122449,\n",
              " 'For': 0.0008163265306122449,\n",
              " 'From': 0.0008163265306122449,\n",
              " 'Gorboduc': 0.0008163265306122449,\n",
              " 'Had': 0.0016326530612244899,\n",
              " 'Hal': 0.0008163265306122449,\n",
              " 'Hamlet': 0.0032653061224489797,\n",
              " 'Harry': 0.005714285714285714,\n",
              " 'Hath': 0.0016326530612244899,\n",
              " 'Have': 0.0008163265306122449,\n",
              " 'Henry': 0.07428571428571429,\n",
              " 'I': 0.0024489795918367346,\n",
              " 'In': 0.0008163265306122449,\n",
              " 'Is': 0.0016326530612244899,\n",
              " 'John': 0.029387755102040815,\n",
              " 'Knows': 0.0008163265306122449,\n",
              " 'Lear': 0.0024489795918367346,\n",
              " 'Leontes': 0.0016326530612244899,\n",
              " 'Lewis': 0.006530612244897959,\n",
              " 'Manchus': 0.0008163265306122449,\n",
              " 'Of': 0.0032653061224489797,\n",
              " 'Pepin': 0.0016326530612244899,\n",
              " 'Pericles': 0.004897959183673469,\n",
              " 'Pharamond': 0.0008163265306122449,\n",
              " 'Philip': 0.007346938775510204,\n",
              " 'Pippen': 0.0016326530612244899,\n",
              " 'Polixenes': 0.0008163265306122449,\n",
              " 'Prettily': 0.0008163265306122449,\n",
              " 'Priam': 0.0024489795918367346,\n",
              " 'Reignier': 0.0008163265306122449,\n",
              " 'Richard': 0.026938775510204082,\n",
              " 'Seems': 0.0008163265306122449,\n",
              " 'Shall': 0.0016326530612244899,\n",
              " 'Should': 0.0016326530612244899,\n",
              " 'Simonides': 0.0032653061224489797,\n",
              " 'Smile': 0.0008163265306122449,\n",
              " 'So': 0.0008163265306122449,\n",
              " 'Stephano': 0.0016326530612244899,\n",
              " 'Stephen': 0.0008163265306122449,\n",
              " 'That': 0.0032653061224489797,\n",
              " 'To': 0.0024489795918367346,\n",
              " 'Toward': 0.0008163265306122449,\n",
              " 'Unto': 0.0008163265306122449,\n",
              " 'Urinal': 0.0008163265306122449,\n",
              " 'Yet': 0.0008163265306122449,\n",
              " ']': 0.017142857142857144,\n",
              " 'a': 0.005714285714285714,\n",
              " 'against': 0.0008163265306122449,\n",
              " 'all': 0.0008163265306122449,\n",
              " 'already': 0.0008163265306122449,\n",
              " 'and': 0.04,\n",
              " 'as': 0.0008163265306122449,\n",
              " 'assisted': 0.0008163265306122449,\n",
              " 'at': 0.0008163265306122449,\n",
              " 'back': 0.0008163265306122449,\n",
              " 'before': 0.0016326530612244899,\n",
              " 'being': 0.0008163265306122449,\n",
              " 'besides': 0.0008163265306122449,\n",
              " 'best': 0.0008163265306122449,\n",
              " 'blame': 0.0008163265306122449,\n",
              " 'bow': 0.0008163265306122449,\n",
              " 'but': 0.0008163265306122449,\n",
              " 'by': 0.0024489795918367346,\n",
              " 'call': 0.0016326530612244899,\n",
              " 'calls': 0.0008163265306122449,\n",
              " 'chooses': 0.0008163265306122449,\n",
              " 'come': 0.0024489795918367346,\n",
              " 'comes': 0.0032653061224489797,\n",
              " 'commands': 0.0008163265306122449,\n",
              " 'concerns': 0.0008163265306122449,\n",
              " 'cried': 0.0008163265306122449,\n",
              " 'dead': 0.0016326530612244899,\n",
              " 'did': 0.0016326530612244899,\n",
              " 'dies': 0.0008163265306122449,\n",
              " 'do': 0.0016326530612244899,\n",
              " 'doth': 0.005714285714285714,\n",
              " 'draws': 0.0008163265306122449,\n",
              " 'drinks': 0.0008163265306122449,\n",
              " 'employed': 0.0008163265306122449,\n",
              " 'enacts': 0.0008163265306122449,\n",
              " 'encamped': 0.0008163265306122449,\n",
              " 'entereth': 0.0008163265306122449,\n",
              " 'enters': 0.0016326530612244899,\n",
              " 'escaped': 0.0008163265306122449,\n",
              " 'exceedeth': 0.0008163265306122449,\n",
              " 'exiled': 0.0008163265306122449,\n",
              " 'exit': 0.0024489795918367346,\n",
              " 'exits': 0.004081632653061225,\n",
              " 'falls': 0.0008163265306122449,\n",
              " 'favors': 0.0008163265306122449,\n",
              " 'first': 0.0008163265306122449,\n",
              " 'for': 0.0016326530612244899,\n",
              " 'forbids': 0.0008163265306122449,\n",
              " 'forth': 0.0008163265306122449,\n",
              " 'found': 0.0008163265306122449,\n",
              " 'from': 0.0024489795918367346,\n",
              " 'gone': 0.0016326530612244899,\n",
              " 'grown': 0.0008163265306122449,\n",
              " 'grows': 0.0008163265306122449,\n",
              " 'guilty': 0.0008163265306122449,\n",
              " 'had': 0.0032653061224489797,\n",
              " 'has': 0.004897959183673469,\n",
              " 'hath': 0.02122448979591837,\n",
              " 'have': 0.0016326530612244899,\n",
              " 'he': 0.0016326530612244899,\n",
              " 'hear': 0.0008163265306122449,\n",
              " 'hence': 0.0008163265306122449,\n",
              " 'her': 0.0016326530612244899,\n",
              " 'here': 0.0008163265306122449,\n",
              " 'himself': 0.00816326530612245,\n",
              " 'his': 0.004897959183673469,\n",
              " 'hold': 0.0008163265306122449,\n",
              " 'in': 0.007346938775510204,\n",
              " 'is': 0.031020408163265307,\n",
              " 'keeps': 0.0008163265306122449,\n",
              " 'kisses': 0.0008163265306122449,\n",
              " 'know': 0.0008163265306122449,\n",
              " 'knows': 0.0008163265306122449,\n",
              " 'lack': 0.0008163265306122449,\n",
              " 'languishes': 0.0008163265306122449,\n",
              " 'like': 0.0008163265306122449,\n",
              " 'loves': 0.0008163265306122449,\n",
              " 'made': 0.0008163265306122449,\n",
              " 'may': 0.0008163265306122449,\n",
              " 'mine': 0.0008163265306122449,\n",
              " 'miscarry': 0.0008163265306122449,\n",
              " 'more': 0.0008163265306122449,\n",
              " 'must': 0.0008163265306122449,\n",
              " 'my': 0.013061224489795919,\n",
              " 'nearest': 0.0008163265306122449,\n",
              " 'nor': 0.0008163265306122449,\n",
              " 'not': 0.0008163265306122449,\n",
              " 'now': 0.0008163265306122449,\n",
              " 'of': 0.06612244897959184,\n",
              " 'once': 0.0008163265306122449,\n",
              " 'or': 0.0016326530612244899,\n",
              " 'our': 0.0016326530612244899,\n",
              " 'papers': 0.0008163265306122449,\n",
              " 'permitted': 0.0008163265306122449,\n",
              " 'please': 0.0008163265306122449,\n",
              " 'reads': 0.0008163265306122449,\n",
              " 'recovers': 0.0008163265306122449,\n",
              " 'reposeth': 0.0008163265306122449,\n",
              " 'returned': 0.0008163265306122449,\n",
              " 'returns': 0.0008163265306122449,\n",
              " 'rises': 0.0008163265306122449,\n",
              " 'riseth': 0.0008163265306122449,\n",
              " 'say': 0.0008163265306122449,\n",
              " 'severely': 0.0008163265306122449,\n",
              " 'shall': 0.008979591836734694,\n",
              " 'she': 0.0008163265306122449,\n",
              " 'should': 0.0008163265306122449,\n",
              " 'shows': 0.0008163265306122449,\n",
              " 'so': 0.0008163265306122449,\n",
              " 'speaks': 0.0008163265306122449,\n",
              " 'stands': 0.0008163265306122449,\n",
              " 'step': 0.0008163265306122449,\n",
              " 'steps': 0.0008163265306122449,\n",
              " 'stirring': 0.0008163265306122449,\n",
              " 't': 0.0008163265306122449,\n",
              " 'takes': 0.0016326530612244899,\n",
              " 'taste': 0.0008163265306122449,\n",
              " 'than': 0.0008163265306122449,\n",
              " 'that': 0.006530612244897959,\n",
              " 'the': 0.0008163265306122449,\n",
              " 'this': 0.0016326530612244899,\n",
              " 'to': 0.0032653061224489797,\n",
              " 'today': 0.0008163265306122449,\n",
              " 'tonight': 0.0008163265306122449,\n",
              " 'unhandled': 0.0008163265306122449,\n",
              " 'unto': 0.0008163265306122449,\n",
              " 'very': 0.0008163265306122449,\n",
              " 'was': 0.0032653061224489797,\n",
              " 'were': 0.0008163265306122449,\n",
              " 'whom': 0.0008163265306122449,\n",
              " 'will': 0.009795918367346938,\n",
              " 'wipes': 0.0008163265306122449,\n",
              " 'with': 0.0032653061224489797,\n",
              " 'withal': 0.0008163265306122449,\n",
              " 'would': 0.0016326530612244899,\n",
              " 'your': 0.00816326530612245}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8Rm6jdgABgw"
      },
      "source": [
        "We are going to use trigrams :\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEm0RK32ADSq"
      },
      "source": [
        "trigram_model = defaultdict(lambda: defaultdict(lambda: 0))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Eq2xCiJAF66"
      },
      "source": [
        "for sentence in tokens:\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        trigram_model[(w1, w2)][w3] += 1"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhDgb4gJALqT"
      },
      "source": [
        "for w1_w2 in trigram_model:\n",
        "    total_count = float(sum(trigram_model[w1_w2].values()))\n",
        "    for w3 in trigram_model[w1_w2]:\n",
        "        trigram_model[w1_w2][w3] /= total_count"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIOCHXMNAV0L",
        "outputId": "112dd4ba-bee3-4892-ec49-2617bade7467"
      },
      "source": [
        "dict(trigram_model[\"the\",\"battle\"])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"'s\": 0.06666666666666667,\n",
              " ',': 0.23333333333333334,\n",
              " '.': 0.1,\n",
              " 'Which': 0.03333333333333333,\n",
              " 'and': 0.06666666666666667,\n",
              " 'came': 0.03333333333333333,\n",
              " 'ends': 0.03333333333333333,\n",
              " 'fly': 0.03333333333333333,\n",
              " \"ha'\": 0.03333333333333333,\n",
              " 'let': 0.03333333333333333,\n",
              " 'of': 0.03333333333333333,\n",
              " 'range': 0.03333333333333333,\n",
              " 'sought': 0.03333333333333333,\n",
              " 'still': 0.03333333333333333,\n",
              " 'than': 0.03333333333333333,\n",
              " 'think': 0.06666666666666667,\n",
              " 'thus': 0.06666666666666667,\n",
              " 'to': 0.03333333333333333}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXGvt-UIAY6h"
      },
      "source": [
        "4) Write a function that takes the following three parameters: model, list of start words, number of sentences to generate. This function should return the sentences generated as a list. DO NOT print anything to the screen from within the function. Use this function to generate 10 sentences with the bigram model from the previous question, and 5 sentences with the trigram model from the previous question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFT7fyFAGofJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_6AmYiGAao6"
      },
      "source": [
        "Bonus (20 points): Using the same methodology from questions 1 and 2, create a similarity matrix between the 20 newsgroups corpus. And \u0000nd the top 5 similar newsgroups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgAzMIpTAfZk"
      },
      "source": [
        "new_newsgroups_data = fetch_20newsgroups()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5HR0oY6AiIW",
        "outputId": "49929518-175b-4f2e-fd76-e8d18908935e"
      },
      "source": [
        "tfidf = TfidfVectorizer()\n",
        "matrix = tfidf.fit_transform(new_newsgroups_data.data)\n",
        "\n",
        "Similarity_Matrix = cosine_similarity(matrix, matrix)\n",
        "print(Similarity_Matrix)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.04405974 0.11017033 ... 0.04433678 0.04457107 0.0329325 ]\n",
            " [0.04405974 1.         0.06242113 ... 0.07373268 0.06959306 0.02439956]\n",
            " [0.11017033 0.06242113 1.         ... 0.07569182 0.06214891 0.02357985]\n",
            " ...\n",
            " [0.04433678 0.07373268 0.07569182 ... 1.         0.02909961 0.00716986]\n",
            " [0.04457107 0.06959306 0.06214891 ... 0.02909961 1.         0.02428174]\n",
            " [0.0329325  0.02439956 0.02357985 ... 0.00716986 0.02428174 1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYh643foAo2n",
        "outputId": "390858dd-89f3-4af6-bded-6d58cef9bfe8"
      },
      "source": [
        "similar_works = best_works(Similarity_Matrix,10)\n",
        "print(similar_works)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(10777, 2002, 1.0000000000000002), (5392, 14, 1.0000000000000002), (9989, 800, 1.0), (4495, 4772, 0.9997809868890166), (8665, 4772, 0.9996809512865921), (981, 9920, 0.999622508768835), (8665, 4495, 0.9995302511028284), (4515, 4495, 0.9992512742959921), (4515, 4772, 0.9990101094182835), (4515, 8665, 0.9989914519713721)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}